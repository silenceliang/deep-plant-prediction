{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/ Valid/ Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['DATA_DIR'] = 'data'\n",
    "args['LABEL_DIR'] = 'label2.csv'\n",
    "args['IMAGE_DIR'] = os.path.join(args['DATA_DIR'], 'DaanForestPark')\n",
    "\n",
    "args['NUM_WORKERS'] = 4\n",
    "args['EPOCHES'] = 50\n",
    "args['BATCH_SIZE'] = 32\n",
    "args['PATIENCE'] = 5\n",
    "args['VALID_RATIO'] = .2\n",
    "args['LR'] = 1e-2\n",
    "args['MIN_LR'] = 1e-5\n",
    "args['L1_ratio'] = 1e-4\n",
    "args['L2_ratio'] = 1e-3\n",
    "args['CLIPPING'] = .9\n",
    "args['W_DECAY'] = .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToPILImage() -> Resize() -> ToTensor()\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "#         transforms.CenterCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "#         transforms.ColorJitter(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])        \n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized DataSet and Official DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        _images, _labels = [], []\n",
    "        # total amount of dataset \n",
    "        _number = 0\n",
    "        # Reading the categorical file\n",
    "        label_df = pd.read_csv(label_dir)\n",
    "        \n",
    "        # Iterate all files including .jpg inages  \n",
    "        for subdir, dirs, files in tqdm(os.walk(image_dir)):\n",
    "            for filename in files:\n",
    "                corr_label = label_df[label_df['dirpath']==subdir[len(args['DATA_DIR'])+1:]]['label'].values\n",
    "                if corr_label.size!= 0 and filename.endswith(('jpg')):\n",
    "                    _images.append(subdir + os.sep + filename)\n",
    "                    _labels.append(corr_label)\n",
    "                    _number+=1\n",
    "        \n",
    "        # Randomly arrange data pairs\n",
    "        mapIndexPosition = list(zip(_images, _labels))\n",
    "        random.shuffle(mapIndexPosition)\n",
    "        _images, _labels = zip(*mapIndexPosition)\n",
    "        \n",
    "        self._image = iter(_images)\n",
    "        self._labels = iter(_labels)\n",
    "        self._number = _number\n",
    "        self._category = label_df['label'].nunique()\n",
    "        self._corr_name2id = {label_df[label_df['label'] == n]['target'].unique()[0]:n for n in label_df['label'].unique()}\n",
    "        self._corr_id2name = {v:k for k,v in self._corr_name2id.items()}\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._number\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        img = next(self._image)\n",
    "        lab = next(self._labels)\n",
    "        \n",
    "        img = self._loadimage(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)        \n",
    "        return img, lab\n",
    "     \n",
    "    def _categorical(self, label):\n",
    "        return np.arange(self._category) == label[:,None]\n",
    "    \n",
    "    def _loadimage(self, file):\n",
    "        return Image.open(file).convert('RGB')\n",
    "    \n",
    "    def get_categorical_nums(self):\n",
    "        return self._category\n",
    "\n",
    "    def get_name2id_dict(self):\n",
    "        return self._corr_name2id\n",
    "    \n",
    "    def get_id2name_dict(self):\n",
    "        return self._corr_id2name    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19168b364e34683926571af5f2d7e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(args['IMAGE_DIR'], args['LABEL_DIR'], transform=transform)\n",
    "\n",
    "valid_size = args['VALID_RATIO']\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args['BATCH_SIZE'], num_workers=args['NUM_WORKERS'], sampler=train_sampler, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=train_dataset, batch_size=args['BATCH_SIZE'], num_workers=args['NUM_WORKERS'], sampler=valid_sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Conv2d:\n",
    "#         nn.init.xavier_uniform(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)   \n",
    "# class SimpleCNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, target):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         # Input size (3, 1136, 640)\n",
    "# #         self.imgzipper  = nn.AvgPool2d(kernel_size=ZIPSIZE)\n",
    "#         # Input size (3, 284, 160)\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=CHANNEL_NUMS,\n",
    "#                         out_channels=FILTER_NUMS,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (8, 1136, 640)\n",
    "#             nn.MaxPool2d(kernel_size=KERNEL_SIZE)\n",
    "#             # (8, 87, 49)\n",
    "#             # zipper (8, 21, 12)\n",
    "#         ).apply(init_weights)\n",
    "        \n",
    "#         # (8, 87, 49)\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=FILTER_NUMS,\n",
    "#                         out_channels=FILTER_NUMS2,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (16, 87, 49)\n",
    "#             nn.MaxPool2d(kernel_size=5)\n",
    "#             # (16, 6, 3)\n",
    "#         ).apply(init_weights)\n",
    "#         self.MLP = nn.Sequential(\n",
    "#             nn.Linear(128, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, target)\n",
    "#         ).apply(init_weights)\n",
    "#     def forward(self, x):\n",
    "#         x = self.imgzipper(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.MLP(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleCNN(train_dataset.get_categorical_nums()).to(device)\n",
    "model = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "# model = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, train_dataset.get_categorical_nums())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 24, 112, 112]             648\n",
      "       BatchNorm2d-2         [-1, 24, 112, 112]              48\n",
      "              ReLU-3         [-1, 24, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 24, 56, 56]               0\n",
      "            Conv2d-5           [-1, 24, 28, 28]             216\n",
      "       BatchNorm2d-6           [-1, 24, 28, 28]              48\n",
      "            Conv2d-7           [-1, 58, 28, 28]           1,392\n",
      "       BatchNorm2d-8           [-1, 58, 28, 28]             116\n",
      "              ReLU-9           [-1, 58, 28, 28]               0\n",
      "           Conv2d-10           [-1, 58, 56, 56]           1,392\n",
      "      BatchNorm2d-11           [-1, 58, 56, 56]             116\n",
      "             ReLU-12           [-1, 58, 56, 56]               0\n",
      "           Conv2d-13           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-14           [-1, 58, 28, 28]             116\n",
      "           Conv2d-15           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-16           [-1, 58, 28, 28]             116\n",
      "             ReLU-17           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-18          [-1, 116, 28, 28]               0\n",
      "           Conv2d-19           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-20           [-1, 58, 28, 28]             116\n",
      "             ReLU-21           [-1, 58, 28, 28]               0\n",
      "           Conv2d-22           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-23           [-1, 58, 28, 28]             116\n",
      "           Conv2d-24           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-25           [-1, 58, 28, 28]             116\n",
      "             ReLU-26           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-27          [-1, 116, 28, 28]               0\n",
      "           Conv2d-28           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-29           [-1, 58, 28, 28]             116\n",
      "             ReLU-30           [-1, 58, 28, 28]               0\n",
      "           Conv2d-31           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-32           [-1, 58, 28, 28]             116\n",
      "           Conv2d-33           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-34           [-1, 58, 28, 28]             116\n",
      "             ReLU-35           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-36          [-1, 116, 28, 28]               0\n",
      "           Conv2d-37           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-38           [-1, 58, 28, 28]             116\n",
      "             ReLU-39           [-1, 58, 28, 28]               0\n",
      "           Conv2d-40           [-1, 58, 28, 28]             522\n",
      "      BatchNorm2d-41           [-1, 58, 28, 28]             116\n",
      "           Conv2d-42           [-1, 58, 28, 28]           3,364\n",
      "      BatchNorm2d-43           [-1, 58, 28, 28]             116\n",
      "             ReLU-44           [-1, 58, 28, 28]               0\n",
      " InvertedResidual-45          [-1, 116, 28, 28]               0\n",
      "           Conv2d-46          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-47          [-1, 116, 14, 14]             232\n",
      "           Conv2d-48          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-49          [-1, 116, 14, 14]             232\n",
      "             ReLU-50          [-1, 116, 14, 14]               0\n",
      "           Conv2d-51          [-1, 116, 28, 28]          13,456\n",
      "      BatchNorm2d-52          [-1, 116, 28, 28]             232\n",
      "             ReLU-53          [-1, 116, 28, 28]               0\n",
      "           Conv2d-54          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-55          [-1, 116, 14, 14]             232\n",
      "           Conv2d-56          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-57          [-1, 116, 14, 14]             232\n",
      "             ReLU-58          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-59          [-1, 232, 14, 14]               0\n",
      "           Conv2d-60          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-61          [-1, 116, 14, 14]             232\n",
      "             ReLU-62          [-1, 116, 14, 14]               0\n",
      "           Conv2d-63          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-64          [-1, 116, 14, 14]             232\n",
      "           Conv2d-65          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-66          [-1, 116, 14, 14]             232\n",
      "             ReLU-67          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-68          [-1, 232, 14, 14]               0\n",
      "           Conv2d-69          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-70          [-1, 116, 14, 14]             232\n",
      "             ReLU-71          [-1, 116, 14, 14]               0\n",
      "           Conv2d-72          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-73          [-1, 116, 14, 14]             232\n",
      "           Conv2d-74          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-75          [-1, 116, 14, 14]             232\n",
      "             ReLU-76          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-77          [-1, 232, 14, 14]               0\n",
      "           Conv2d-78          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-79          [-1, 116, 14, 14]             232\n",
      "             ReLU-80          [-1, 116, 14, 14]               0\n",
      "           Conv2d-81          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-82          [-1, 116, 14, 14]             232\n",
      "           Conv2d-83          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-84          [-1, 116, 14, 14]             232\n",
      "             ReLU-85          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-86          [-1, 232, 14, 14]               0\n",
      "           Conv2d-87          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-88          [-1, 116, 14, 14]             232\n",
      "             ReLU-89          [-1, 116, 14, 14]               0\n",
      "           Conv2d-90          [-1, 116, 14, 14]           1,044\n",
      "      BatchNorm2d-91          [-1, 116, 14, 14]             232\n",
      "           Conv2d-92          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-93          [-1, 116, 14, 14]             232\n",
      "             ReLU-94          [-1, 116, 14, 14]               0\n",
      " InvertedResidual-95          [-1, 232, 14, 14]               0\n",
      "           Conv2d-96          [-1, 116, 14, 14]          13,456\n",
      "      BatchNorm2d-97          [-1, 116, 14, 14]             232\n",
      "             ReLU-98          [-1, 116, 14, 14]               0\n",
      "           Conv2d-99          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-100          [-1, 116, 14, 14]             232\n",
      "          Conv2d-101          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-102          [-1, 116, 14, 14]             232\n",
      "            ReLU-103          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-104          [-1, 232, 14, 14]               0\n",
      "          Conv2d-105          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-106          [-1, 116, 14, 14]             232\n",
      "            ReLU-107          [-1, 116, 14, 14]               0\n",
      "          Conv2d-108          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-109          [-1, 116, 14, 14]             232\n",
      "          Conv2d-110          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-111          [-1, 116, 14, 14]             232\n",
      "            ReLU-112          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-113          [-1, 232, 14, 14]               0\n",
      "          Conv2d-114          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-115          [-1, 116, 14, 14]             232\n",
      "            ReLU-116          [-1, 116, 14, 14]               0\n",
      "          Conv2d-117          [-1, 116, 14, 14]           1,044\n",
      "     BatchNorm2d-118          [-1, 116, 14, 14]             232\n",
      "          Conv2d-119          [-1, 116, 14, 14]          13,456\n",
      "     BatchNorm2d-120          [-1, 116, 14, 14]             232\n",
      "            ReLU-121          [-1, 116, 14, 14]               0\n",
      "InvertedResidual-122          [-1, 232, 14, 14]               0\n",
      "          Conv2d-123            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-124            [-1, 232, 7, 7]             464\n",
      "          Conv2d-125            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-126            [-1, 232, 7, 7]             464\n",
      "            ReLU-127            [-1, 232, 7, 7]               0\n",
      "          Conv2d-128          [-1, 232, 14, 14]          53,824\n",
      "     BatchNorm2d-129          [-1, 232, 14, 14]             464\n",
      "            ReLU-130          [-1, 232, 14, 14]               0\n",
      "          Conv2d-131            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-132            [-1, 232, 7, 7]             464\n",
      "          Conv2d-133            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-134            [-1, 232, 7, 7]             464\n",
      "            ReLU-135            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-136            [-1, 464, 7, 7]               0\n",
      "          Conv2d-137            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-138            [-1, 232, 7, 7]             464\n",
      "            ReLU-139            [-1, 232, 7, 7]               0\n",
      "          Conv2d-140            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-141            [-1, 232, 7, 7]             464\n",
      "          Conv2d-142            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-143            [-1, 232, 7, 7]             464\n",
      "            ReLU-144            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-145            [-1, 464, 7, 7]               0\n",
      "          Conv2d-146            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-147            [-1, 232, 7, 7]             464\n",
      "            ReLU-148            [-1, 232, 7, 7]               0\n",
      "          Conv2d-149            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-150            [-1, 232, 7, 7]             464\n",
      "          Conv2d-151            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-152            [-1, 232, 7, 7]             464\n",
      "            ReLU-153            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-154            [-1, 464, 7, 7]               0\n",
      "          Conv2d-155            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-156            [-1, 232, 7, 7]             464\n",
      "            ReLU-157            [-1, 232, 7, 7]               0\n",
      "          Conv2d-158            [-1, 232, 7, 7]           2,088\n",
      "     BatchNorm2d-159            [-1, 232, 7, 7]             464\n",
      "          Conv2d-160            [-1, 232, 7, 7]          53,824\n",
      "     BatchNorm2d-161            [-1, 232, 7, 7]             464\n",
      "            ReLU-162            [-1, 232, 7, 7]               0\n",
      "InvertedResidual-163            [-1, 464, 7, 7]               0\n",
      "          Conv2d-164           [-1, 1024, 7, 7]         475,136\n",
      "     BatchNorm2d-165           [-1, 1024, 7, 7]           2,048\n",
      "            ReLU-166           [-1, 1024, 7, 7]               0\n",
      "          Linear-167                   [-1, 85]          87,125\n",
      "================================================================\n",
      "Total params: 1,340,729\n",
      "Trainable params: 1,340,729\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 47.93\n",
      "Params size (MB): 5.11\n",
      "Estimated Total Size (MB): 53.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# channels, H, W\n",
    "model = model.to(device=device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically tunning Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 根據epoch調整 lr \"\"\"\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args['LR'] * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 分層設定 lr '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 分層設定 lr \"\"\"\n",
    "# large_lr_layers = list(map(id,model.fc.parameters()))\n",
    "# small_lr_layers = filter(lambda p:id(p) not in large_lr_layers,model.parameters())\n",
    "# optimizer = torch.optim.SGD([\n",
    "#             {\"params\":large_lr_layers},\n",
    "#             {\"params\":small_lr_layers,\"lr\":1e-4}\n",
    "#             ],lr = 1e-2,momenum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, state, filename, ckptname='resNet.ckpt'):\n",
    "    if not os.path.isdir(filename):\n",
    "        try: \n",
    "            os.mkdir(filename) \n",
    "            print('Create {}'.format(filename))\n",
    "        except OSError as err: \n",
    "            raise err\n",
    "\n",
    "    _ckpt = os.path.join(filename, ckptname)\n",
    "    torch.save(state, _ckpt)\n",
    "    print('Saving the {} in {}'.format(ckptname, filename))\n",
    "\n",
    "    \"\"\" If you wwanna save the whole model, Uncomment below \"\"\"\n",
    "#     _model = os.path.join(filename, 'model_best.ckpt')\n",
    "#     torch.save(model, _model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_confusion(writer, step, matrix, class_dict):\n",
    "    \"\"\"\n",
    "    Visualization of confusion matrix\n",
    "\n",
    "    Parameters:\n",
    "        writer (tensorboard.SummaryWriter): TensorBoard SummaryWriter instance.\n",
    "        step (int): Counter usually specifying steps/epochs/time.\n",
    "        matrix (numpy.array): Square-shaped array of size class x class.\n",
    "            Should specify cross-class accuracies/confusion in percent\n",
    "            values (range 0-1).\n",
    "        class_dict (dict): Dictionary specifying class names as keys and\n",
    "            corresponding integer labels/targets as values.\n",
    "    \"\"\"\n",
    "    all_categories = sorted(class_dict, key=class_dict.get)\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    matrix = matrix.astype(float)\n",
    "    for i in range(len(class_dict)):\n",
    "        matrix[i] = matrix[i] / matrix[i].sum()\n",
    "\n",
    "    # Create the figure\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    # Show the matrix and define a discretized color bar\n",
    "    cax = ax.matshow(matrix, cmap=plt.cm.get_cmap('Oranges'))\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes. Rotate the x ticks by 90 degrees.\n",
    "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_categories)\n",
    "    \n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    thresh = matrix.max() / 2\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, matrix[i, j],\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    \n",
    "    # Turn off the grid for this plot. Enforce a tight layout to reduce white margins\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Call our auxiliary to TensorBoard function to render the figure \n",
    "    plot_to_tensorboard(writer, fig, step)\n",
    "    \n",
    "    \n",
    "def plot_to_tensorboard(writer, fig, step):\n",
    "    \"\"\"\n",
    "    Takes a matplotlib figure handle and converts it using\n",
    "    canvas and string-casts to a numpy array that can be\n",
    "    visualized in TensorBoard using the add_image function\n",
    "\n",
    "    Parameters:\n",
    "        writer (tensorboard.SummaryWriter): TensorBoard SummaryWriter instance.\n",
    "        fig (matplotlib.pyplot.fig): Matplotlib figure handle.\n",
    "        step (int): counter usually specifying steps/epochs/time.\n",
    "    \"\"\"\n",
    "\n",
    "    # Draw figure on canvas\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Convert the figure to numpy array, read the pixel values and reshape the array\n",
    "    img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    img = img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    # Normalize into 0-1 range for TensorBoard(X). Swap axes for newer versions where API expects colors in first dim\n",
    "    img = img / 255.0\n",
    "    img = np.transpose(img, (2,0,1))\n",
    "    # img = np.swapaxes(img, 0, 2) # if your TensorFlow + TensorBoard version are >= 1.8\n",
    "    # Add figure in numpy \"image\" to TensorBoard writer\n",
    "    writer.add_image('valid/confusion_matrix', img, step)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b91c4a2e8149358ba23b24ce8b6a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0,             total_loss=1.019,             accuracy=0.938\n",
      "epoch 1, step 50,             total_loss=0.689,             accuracy=0.969\n",
      "epoch 1, step 100,             total_loss=0.866,             accuracy=0.906\n",
      "epoch 1, step 150,             total_loss=1.051,             accuracy=0.875\n",
      "epoch 1, step 200,             total_loss=0.876,             accuracy=0.906\n",
      "epoch 1, step 250,             total_loss=0.723,             accuracy=0.969\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:70: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, val_loss=0.512\n",
      "Create runs/experiment_836147/ckpt\n",
      "Saving the resNet.ckpt in runs/experiment_836147/ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672793e7bf4c4813907b6953cdeb9e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 0,             total_loss=1.071,             accuracy=0.875\n",
      "epoch 2, step 50,             total_loss=0.738,             accuracy=0.938\n",
      "epoch 2, step 100,             total_loss=0.926,             accuracy=0.875\n",
      "epoch 2, step 150,             total_loss=0.863,             accuracy=0.906\n",
      "epoch 2, step 200,             total_loss=1.218,             accuracy=0.781\n",
      "epoch 2, step 250,             total_loss=0.967,             accuracy=0.906\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "Epoch     2: reducing learning rate of group 0 to 9.0000e-03.\n",
      "epoch 2, val_loss=0.514\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcd8dd5a4474241b8b896817a641b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 0,             total_loss=0.892,             accuracy=0.875\n",
      "epoch 3, step 50,             total_loss=0.663,             accuracy=1.000\n",
      "epoch 3, step 100,             total_loss=0.805,             accuracy=0.906\n",
      "epoch 3, step 150,             total_loss=1.019,             accuracy=0.875\n",
      "epoch 3, step 200,             total_loss=0.917,             accuracy=0.875\n",
      "epoch 3, step 250,             total_loss=0.678,             accuracy=0.969\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "epoch 3, val_loss=0.327\n",
      "Saving the resNet.ckpt in runs/experiment_836147/ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfabd6e6b57c44b38ffc2aacad2fd3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 0,             total_loss=0.816,             accuracy=0.938\n",
      "epoch 4, step 50,             total_loss=0.698,             accuracy=0.969\n",
      "epoch 4, step 100,             total_loss=0.681,             accuracy=1.000\n",
      "epoch 4, step 150,             total_loss=0.727,             accuracy=0.969\n",
      "epoch 4, step 200,             total_loss=0.724,             accuracy=0.938\n",
      "epoch 4, step 250,             total_loss=0.704,             accuracy=0.938\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "Epoch     4: reducing learning rate of group 0 to 8.1000e-03.\n",
      "epoch 4, val_loss=0.348\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505d9fc1f9f04d3886aaa421bfd406ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 0,             total_loss=1.151,             accuracy=0.812\n",
      "epoch 5, step 50,             total_loss=0.742,             accuracy=0.938\n",
      "epoch 5, step 100,             total_loss=0.792,             accuracy=0.938\n",
      "epoch 5, step 150,             total_loss=0.747,             accuracy=0.969\n",
      "epoch 5, step 200,             total_loss=0.700,             accuracy=0.906\n",
      "epoch 5, step 250,             total_loss=0.641,             accuracy=0.969\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "epoch 5, val_loss=0.261\n",
      "Saving the resNet.ckpt in runs/experiment_836147/ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1456c35af26419cb0014c2dfbb62b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, step 0,             total_loss=0.710,             accuracy=0.906\n",
      "epoch 6, step 50,             total_loss=0.657,             accuracy=0.969\n",
      "epoch 6, step 100,             total_loss=0.629,             accuracy=0.938\n",
      "epoch 6, step 150,             total_loss=0.737,             accuracy=0.906\n",
      "epoch 6, step 200,             total_loss=0.858,             accuracy=0.906\n",
      "epoch 6, step 250,             total_loss=0.723,             accuracy=0.906\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "Epoch     6: reducing learning rate of group 0 to 7.2900e-03.\n",
      "epoch 6, val_loss=0.363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7d7e516ae748e080f661b3f7341130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, step 0,             total_loss=0.765,             accuracy=0.906\n",
      "epoch 7, step 50,             total_loss=0.725,             accuracy=0.906\n",
      "epoch 7, step 100,             total_loss=0.606,             accuracy=1.000\n",
      "epoch 7, step 150,             total_loss=0.687,             accuracy=0.906\n",
      "epoch 7, step 200,             total_loss=0.634,             accuracy=0.969\n",
      "epoch 7, step 250,             total_loss=0.576,             accuracy=0.969\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "epoch 7, val_loss=0.197\n",
      "Saving the resNet.ckpt in runs/experiment_836147/ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ba2e8c2c534f1d82e5851fe75c6d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, step 0,             total_loss=0.665,             accuracy=0.969\n",
      "epoch 8, step 50,             total_loss=0.630,             accuracy=0.969\n",
      "epoch 8, step 100,             total_loss=0.607,             accuracy=0.969\n",
      "epoch 8, step 150,             total_loss=0.700,             accuracy=0.969\n",
      "epoch 8, step 200,             total_loss=0.772,             accuracy=0.875\n",
      "epoch 8, step 250,             total_loss=0.588,             accuracy=1.000\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "Epoch     8: reducing learning rate of group 0 to 6.5610e-03.\n",
      "epoch 8, val_loss=0.361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777c173989e3449eaa47cb4df7251f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, step 0,             total_loss=0.624,             accuracy=0.969\n",
      "epoch 9, step 50,             total_loss=0.769,             accuracy=0.875\n",
      "epoch 9, step 100,             total_loss=0.559,             accuracy=1.000\n",
      "epoch 9, step 150,             total_loss=0.780,             accuracy=0.844\n",
      "epoch 9, step 200,             total_loss=0.642,             accuracy=0.969\n",
      "epoch 9, step 250,             total_loss=0.635,             accuracy=0.938\n",
      "\n",
      "--- Validation phase ---\n",
      "confusion matrix drawing\n",
      "Epoch     9: reducing learning rate of group 0 to 5.9049e-03.\n",
      "epoch 9, val_loss=0.261\n",
      "Early stopping\n",
      "Finish all training !\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['LR'])   # optimize all cnn parameters\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                       factor=args['W_DECAY'], \n",
    "                                                       patience=0, \n",
    "                                                       min_lr=args['MIN_LR'], \n",
    "                                                       verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "# early stopping\n",
    "min_val_loss = np.Inf\n",
    "patience = args['PATIENCE']\n",
    "global_step = 1\n",
    "# build the writer file\n",
    "write_file = 'runs/experiment_{}'.format(datetime.now().strftime('%f'))\n",
    "writer = SummaryWriter(write_file)\n",
    "# define the ckpt path\n",
    "writer_ckpt_path = os.path.join(write_file, 'ckpt')\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(args['EPOCHES']):\n",
    "    for i, (img_batch, label_batch) in tqdm(enumerate(train_loader)):    \n",
    "        optimizer.zero_grad()      \n",
    "        img_batch = img_batch.to(device=device)\n",
    "        label_batch = label_batch.to(device=device)  \n",
    "        output = model(img_batch)\n",
    "        loss = criterion(output, label_batch.squeeze())\n",
    "        \n",
    "        # l2 Regularization loss\n",
    "        l1_regularization = 0\n",
    "        l2_regularization = 0\n",
    "        for p in model.parameters():\n",
    "            l1_regularization += torch.norm(p, 1)\n",
    "            l2_regularization += torch.norm(p, 2)\n",
    "        loss = loss + (args['L2_ratio'] * l2_regularization)\n",
    "\n",
    "        loss.backward()\n",
    "#         clip the grandient value for avoiding explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), args['CLIPPING']) \n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(output.cpu().data, 1)\n",
    "        accuracy = torch.sum(predicted == label_batch.cpu().data.view(-1), dtype=torch.float32) / args['BATCH_SIZE']\n",
    "        \n",
    "        # Write tensorboard\n",
    "        writer.add_scalar('train/Accuracy', accuracy.item(), global_step)\n",
    "        writer.add_scalar('train/Loss', loss.item(), global_step)\n",
    "        writer.add_scalar('train/L1RegLoss', l1_regularization.item(), global_step)\n",
    "        writer.add_scalar('train/L2RegLoss', l2_regularization.item(), global_step)\n",
    "        writer.add_scalar('train/LR', get_lr(optimizer), global_step)\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "        if i % 50== 0:\n",
    "            print('epoch {}, step {}, \\\n",
    "            total_loss={:.3f}, \\\n",
    "            accuracy={:.3f}'.format(epoch+1, i, loss.item(), accuracy.item()))\n",
    "    \n",
    "    \n",
    "    print('--- Validation phase ---')\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    # classifier recored\n",
    "    val_actual_record = {i:0 for i in range(train_dataset.get_categorical_nums())}\n",
    "    val_pred_record = val_actual_record.copy()\n",
    "    true_positive_record = val_actual_record.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        numpy_label_batch, numpy_predicted = None, None\n",
    "        for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "            output = model(img_batch.to(device))\n",
    "            _, predicted = torch.max(output.cpu().data, 1)\n",
    "            loss = criterion(output, label_batch.to(device).squeeze())\n",
    "            accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / args['BATCH_SIZE']\n",
    "            eval_loss += loss.item()\n",
    "            eval_acc += accuracy.item()\n",
    "            # Write to tensorboard\n",
    "#             for cat in range(train_dataset.get_categorical_nums()):\n",
    "#                 writer.add_pr_curve('valid/pr_curve_{}'.format(cat), (label_batch == cat).int().squeeze(),\n",
    "#                                     (predicted == cat).int().squeeze(), epoch*len(valid_loader)+i)\n",
    "#             writer.add_images('valid/image_batch', img_batch, epoch*len(valid_loader)+i)\n",
    "            \n",
    "            # compute confusion matrix\n",
    "            numpy_label_batch = label_batch.squeeze().data.numpy()\n",
    "            numpy_predicted  = predicted.data.numpy()\n",
    "            \n",
    "            # compute precision and recall\n",
    "            for i, j in zip(numpy_label_batch, numpy_predicted):\n",
    "                if i == j:\n",
    "                    true_positive_record[i]+=1\n",
    "                val_actual_record[i]+=1\n",
    "                val_pred_record[i]+=1\n",
    "            \n",
    "            writer.add_histogram('valid/actual', label_batch, epoch*len(valid_loader)+i)\n",
    "            writer.add_histogram('valid/pred', predicted, epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Accuracy', accuracy.item(), epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Loss', loss.item(), epoch*len(valid_loader)+i)\n",
    "        \n",
    "        print(\"confusion matrix drawing\")\n",
    "        cm = confusion_matrix(numpy_label_batch, numpy_predicted)\n",
    "        batch_dict = { i:train_dataset.get_id2name_dict()[i] \n",
    "                 for i in np.unique(np.concatenate([numpy_label_batch, numpy_predicted])).astype(int)}\n",
    "        # write confusion matrix to tensorboard \n",
    "        vis_confusion(writer, epoch*len(valid_loader)+i, cm, batch_dict)\n",
    "    \n",
    "    \n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    for k in true_positive_record.keys():\n",
    "        precision[train_dataset.get_id2name_dict()[k]] = true_positive_record[k] / (val_pred_record[k] + 0.1)\n",
    "        recall[train_dataset.get_id2name_dict()[k]] = true_positive_record[k] / (val_actual_record[k] + 0.1)\n",
    "    writer.add_scalars('precision', precision, epoch+1)\n",
    "    writer.add_scalars('recall', recall, epoch+1)\n",
    "    \n",
    "    eval_loss = eval_loss / len(valid_loader)\n",
    "    eval_acc = eval_acc / len(valid_loader)\n",
    "\n",
    "    scheduler.step(eval_loss)\n",
    "    \n",
    "    writer.add_hparams(args, {'hparam/epoch': epoch+1,\n",
    "                              'hparam/lr_':get_lr(optimizer),\n",
    "                              'hparam/eval_loss':eval_loss,\n",
    "                              'hparam/accuracy':eval_acc})\n",
    "    print('epoch {}, val_loss={:.3f}'.format(epoch+1, eval_loss))\n",
    "\n",
    "    ## Early Stopping\n",
    "    if eval_loss < min_val_loss:\n",
    "        save_checkpoint(model, \n",
    "            {\n",
    "            'epoch': epoch+1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': eval_loss,\n",
    "            'optimizer' :optimizer.state_dict(),\n",
    "            }, writer_ckpt_path)\n",
    "        min_val_loss = eval_loss\n",
    "    else:\n",
    "        patience-=1\n",
    "    \n",
    "    if patience == 0:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "         \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finish all training !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.9116848111152649\n"
     ]
    }
   ],
   "source": [
    "# CKPT_PATH = 'runs/experiment_837515/ckpt/resNet.ckpt'\n",
    "# model.load_state_dict(torch.load(CKPT_PATH)['state_dict'])\n",
    "model.eval()\n",
    "acc = 0\n",
    "for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "    output = model(img_batch.to(device))\n",
    "    _, predicted = torch.max(output.cpu().data, 1)\n",
    "    accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / args['BATCH_SIZE']\n",
    "    acc += accuracy\n",
    "print('accuracy={}'.format(acc/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the whole model consisting in architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'arch_w_dict.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "# model2 = torch.load('arch_w_dict.ckpt')\n",
    "# model2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release CUDA cache memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From dict to the whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'kaggle/runs/experiment_505653/ckpt/resNet.ckpt'\n",
    "weights = torch.load(path)['state_dict']\n",
    "model.load_state_dict(weights)\n",
    "\n",
    "# torch.save(model, 'best_model96.pt')\n",
    "# torch.save(model, 'best_model96.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
