{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "LABEL_DIR = 'label2.csv'\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, 'DaanForestPark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 20\n",
    "FILTER_NUMS = 8\n",
    "FILTER_NUMS2 = 16\n",
    "CHANNEL_NUMS = 3\n",
    "KERNEL_SIZE = 13\n",
    "STRIDE = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToPILImage() -> Resize() -> ToTensor()\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "#         transforms.ColorJitter(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])        \n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DataSet and the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        _images, _labels = [], []\n",
    "        # total amount of dataset \n",
    "        _number = 0\n",
    "        # Reading the categorical file\n",
    "        label_df = pd.read_csv(label_dir)\n",
    "        \n",
    "        # Iterate all files including .jpg inages  \n",
    "        for subdir, dirs, files in tqdm(os.walk(image_dir)):\n",
    "            for filename in files:\n",
    "                corr_label = label_df[label_df['dirpath']==subdir[len(DATA_DIR)+1:]]['label'].values\n",
    "                if corr_label.size!= 0 and filename.endswith(('jpg')):\n",
    "                    _images.append(subdir + os.sep + filename)\n",
    "                    _labels.append(corr_label)\n",
    "                    _number+=1\n",
    "        \n",
    "        # Randomly arrange data pairs\n",
    "        mapIndexPosition = list(zip(_images, _labels))\n",
    "        random.shuffle(mapIndexPosition)\n",
    "        _images, _labels = zip(*mapIndexPosition)\n",
    "\n",
    "        self._image = iter(_images)\n",
    "        self._labels = iter(_labels)\n",
    "        self._number = _number\n",
    "        self._category = label_df['label'].nunique()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._number\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        img = next(self._image)\n",
    "        lab = next(self._labels)\n",
    "        \n",
    "        img = self._loadimage(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)        \n",
    "        return img, lab\n",
    "     \n",
    "    def _categorical(self, label):\n",
    "        return np.arange(self._category) == label[:,None]\n",
    "    \n",
    "    def _loadimage(self, file):\n",
    "        return Image.open(file).convert('RGB')\n",
    "    \n",
    "    def get_categorical_nums(self):\n",
    "        return self._category\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f33218b490c4ab5967c755410aa828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(IMAGE_DIR, LABEL_DIR, transform=transform)\n",
    "\n",
    "valid_size = .2\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, sampler=train_sampler, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, sampler=valid_sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Conv2d:\n",
    "#         nn.init.xavier_uniform(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)   \n",
    "# class SimpleCNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, target):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         # Input size (3, 1136, 640)\n",
    "# #         self.imgzipper  = nn.AvgPool2d(kernel_size=ZIPSIZE)\n",
    "#         # Input size (3, 284, 160)\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=CHANNEL_NUMS,\n",
    "#                         out_channels=FILTER_NUMS,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (8, 1136, 640)\n",
    "#             nn.MaxPool2d(kernel_size=KERNEL_SIZE)\n",
    "#             # (8, 87, 49)\n",
    "#             # zipper (8, 21, 12)\n",
    "#         ).apply(init_weights)\n",
    "        \n",
    "#         # (8, 87, 49)\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=FILTER_NUMS,\n",
    "#                         out_channels=FILTER_NUMS2,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (16, 87, 49)\n",
    "#             nn.MaxPool2d(kernel_size=5)\n",
    "#             # (16, 6, 3)\n",
    "#         ).apply(init_weights)\n",
    "#         self.MLP = nn.Sequential(\n",
    "#             nn.Linear(128, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, target)\n",
    "#         ).apply(init_weights)\n",
    "#     def forward(self, x):\n",
    "#         x = self.imgzipper(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.MLP(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleCNN(train_dataset.get_categorical_nums()).to(device)\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, train_dataset.get_categorical_nums())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 170, 96]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 170, 96]             128\n",
      "              ReLU-3          [-1, 64, 170, 96]               0\n",
      "         MaxPool2d-4           [-1, 64, 85, 48]               0\n",
      "            Conv2d-5           [-1, 64, 85, 48]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 85, 48]             128\n",
      "              ReLU-7           [-1, 64, 85, 48]               0\n",
      "            Conv2d-8           [-1, 64, 85, 48]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 85, 48]             128\n",
      "             ReLU-10           [-1, 64, 85, 48]               0\n",
      "       BasicBlock-11           [-1, 64, 85, 48]               0\n",
      "           Conv2d-12           [-1, 64, 85, 48]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 85, 48]             128\n",
      "             ReLU-14           [-1, 64, 85, 48]               0\n",
      "           Conv2d-15           [-1, 64, 85, 48]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 85, 48]             128\n",
      "             ReLU-17           [-1, 64, 85, 48]               0\n",
      "       BasicBlock-18           [-1, 64, 85, 48]               0\n",
      "           Conv2d-19          [-1, 128, 43, 24]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 43, 24]             256\n",
      "             ReLU-21          [-1, 128, 43, 24]               0\n",
      "           Conv2d-22          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 43, 24]             256\n",
      "           Conv2d-24          [-1, 128, 43, 24]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 43, 24]             256\n",
      "             ReLU-26          [-1, 128, 43, 24]               0\n",
      "       BasicBlock-27          [-1, 128, 43, 24]               0\n",
      "           Conv2d-28          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 43, 24]             256\n",
      "             ReLU-30          [-1, 128, 43, 24]               0\n",
      "           Conv2d-31          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 43, 24]             256\n",
      "             ReLU-33          [-1, 128, 43, 24]               0\n",
      "       BasicBlock-34          [-1, 128, 43, 24]               0\n",
      "           Conv2d-35          [-1, 256, 22, 12]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 22, 12]             512\n",
      "             ReLU-37          [-1, 256, 22, 12]               0\n",
      "           Conv2d-38          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 22, 12]             512\n",
      "           Conv2d-40          [-1, 256, 22, 12]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 22, 12]             512\n",
      "             ReLU-42          [-1, 256, 22, 12]               0\n",
      "       BasicBlock-43          [-1, 256, 22, 12]               0\n",
      "           Conv2d-44          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 22, 12]             512\n",
      "             ReLU-46          [-1, 256, 22, 12]               0\n",
      "           Conv2d-47          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 22, 12]             512\n",
      "             ReLU-49          [-1, 256, 22, 12]               0\n",
      "       BasicBlock-50          [-1, 256, 22, 12]               0\n",
      "           Conv2d-51           [-1, 512, 11, 6]       1,179,648\n",
      "      BatchNorm2d-52           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-53           [-1, 512, 11, 6]               0\n",
      "           Conv2d-54           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-55           [-1, 512, 11, 6]           1,024\n",
      "           Conv2d-56           [-1, 512, 11, 6]         131,072\n",
      "      BatchNorm2d-57           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-58           [-1, 512, 11, 6]               0\n",
      "       BasicBlock-59           [-1, 512, 11, 6]               0\n",
      "           Conv2d-60           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-61           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-62           [-1, 512, 11, 6]               0\n",
      "           Conv2d-63           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-64           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-65           [-1, 512, 11, 6]               0\n",
      "       BasicBlock-66           [-1, 512, 11, 6]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 85]          43,605\n",
      "================================================================\n",
      "Total params: 11,220,117\n",
      "Trainable params: 11,220,117\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 82.29\n",
      "Params size (MB): 42.80\n",
      "Estimated Total Size (MB): 125.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# channels, H, W\n",
    "model = model_ft.to(device=device)\n",
    "summary(model, input_size=(CHANNEL_NUMS, 340, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "def save_checkpoint(state, filename='ckpt/experiment/model.ckpt'):\n",
    "    torch.save(state, filename)\n",
    "    shutil.copyfile(filename, 'model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ab0f78f9fb4203a3bbe427f6be77b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    w.join()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0,             l2_loss=498.729, total_loss=3.501,             accuracy=0.094\n",
      "epoch 1, step 50,             l2_loss=1032.029, total_loss=4.039,             accuracy=0.000\n",
      "epoch 1, step 100,             l2_loss=1153.462, total_loss=4.069,             accuracy=0.125\n",
      "epoch 1, step 150,             l2_loss=1189.826, total_loss=4.079,             accuracy=0.062\n",
      "epoch 1, step 200,             l2_loss=1209.372, total_loss=4.199,             accuracy=0.094\n",
      "epoch 1, step 250,             l2_loss=1234.380, total_loss=3.965,             accuracy=0.094\n",
      "epoch 1, step 300,             l2_loss=1281.459, total_loss=3.769,             accuracy=0.031\n",
      "epoch 1, step 350,             l2_loss=1333.341, total_loss=3.288,             accuracy=0.188\n",
      "epoch 1, step 400,             l2_loss=1382.245, total_loss=3.586,             accuracy=0.031\n",
      "epoch 1, step 450,             l2_loss=1424.712, total_loss=3.621,             accuracy=0.125\n",
      "epoch 1, step 500,             l2_loss=1463.885, total_loss=3.476,             accuracy=0.125\n",
      "\n",
      "--- Validation phase ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f01b3896d08>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 961, in __del__\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 941, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "  File \"/home/silence/.pyenv/versions/3.7.2/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, val_loss=3.281\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5a453e49e94528b2d511419b9b94c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 0,             l2_loss=1505.968, total_loss=3.515,             accuracy=0.219\n",
      "epoch 2, step 50,             l2_loss=1544.397, total_loss=2.982,             accuracy=0.312\n",
      "epoch 2, step 100,             l2_loss=1584.044, total_loss=3.270,             accuracy=0.125\n",
      "epoch 2, step 150,             l2_loss=1618.714, total_loss=3.341,             accuracy=0.094\n",
      "epoch 2, step 200,             l2_loss=1678.672, total_loss=3.352,             accuracy=0.125\n",
      "epoch 2, step 250,             l2_loss=1744.354, total_loss=3.354,             accuracy=0.125\n",
      "epoch 2, step 300,             l2_loss=1788.474, total_loss=3.238,             accuracy=0.156\n",
      "epoch 2, step 350,             l2_loss=1824.340, total_loss=2.883,             accuracy=0.250\n",
      "epoch 2, step 400,             l2_loss=1868.870, total_loss=3.181,             accuracy=0.219\n",
      "epoch 2, step 450,             l2_loss=1900.854, total_loss=3.291,             accuracy=0.219\n",
      "epoch 2, step 500,             l2_loss=1933.787, total_loss=3.208,             accuracy=0.250\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 2, val_loss=2.902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4a4af333e54f8396b3fb6d76a31ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, step 0,             l2_loss=1967.072, total_loss=2.860,             accuracy=0.281\n",
      "epoch 3, step 50,             l2_loss=2001.847, total_loss=2.559,             accuracy=0.312\n",
      "epoch 3, step 100,             l2_loss=2037.356, total_loss=2.853,             accuracy=0.125\n",
      "epoch 3, step 150,             l2_loss=2067.986, total_loss=2.792,             accuracy=0.219\n",
      "epoch 3, step 200,             l2_loss=2100.204, total_loss=2.726,             accuracy=0.281\n",
      "epoch 3, step 250,             l2_loss=2127.115, total_loss=3.177,             accuracy=0.219\n",
      "epoch 3, step 300,             l2_loss=2161.536, total_loss=2.530,             accuracy=0.312\n",
      "epoch 3, step 350,             l2_loss=2187.573, total_loss=2.129,             accuracy=0.406\n",
      "epoch 3, step 400,             l2_loss=2220.009, total_loss=2.484,             accuracy=0.250\n",
      "epoch 3, step 450,             l2_loss=2245.386, total_loss=2.859,             accuracy=0.219\n",
      "epoch 3, step 500,             l2_loss=2273.683, total_loss=2.280,             accuracy=0.344\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 3, val_loss=2.506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c98410131d4881a0aa278d2122700b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, step 0,             l2_loss=2301.588, total_loss=2.566,             accuracy=0.375\n",
      "epoch 4, step 50,             l2_loss=2330.956, total_loss=2.095,             accuracy=0.375\n",
      "epoch 4, step 100,             l2_loss=2356.584, total_loss=2.607,             accuracy=0.250\n",
      "epoch 4, step 150,             l2_loss=2383.306, total_loss=2.315,             accuracy=0.375\n",
      "epoch 4, step 200,             l2_loss=2415.496, total_loss=2.057,             accuracy=0.406\n",
      "epoch 4, step 250,             l2_loss=2440.919, total_loss=2.753,             accuracy=0.250\n",
      "epoch 4, step 300,             l2_loss=2472.382, total_loss=2.202,             accuracy=0.438\n",
      "epoch 4, step 350,             l2_loss=2496.306, total_loss=1.871,             accuracy=0.469\n",
      "epoch 4, step 400,             l2_loss=2521.280, total_loss=1.980,             accuracy=0.375\n",
      "epoch 4, step 450,             l2_loss=2541.902, total_loss=2.566,             accuracy=0.281\n",
      "epoch 4, step 500,             l2_loss=2569.971, total_loss=1.906,             accuracy=0.281\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 4, val_loss=2.231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9639dadb04472d980a6cc16e0b6724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, step 0,             l2_loss=2605.767, total_loss=2.331,             accuracy=0.375\n",
      "epoch 5, step 50,             l2_loss=2634.507, total_loss=1.651,             accuracy=0.500\n",
      "epoch 5, step 100,             l2_loss=2668.616, total_loss=2.166,             accuracy=0.312\n",
      "epoch 5, step 150,             l2_loss=2691.591, total_loss=1.884,             accuracy=0.281\n",
      "epoch 5, step 200,             l2_loss=2712.852, total_loss=1.884,             accuracy=0.438\n",
      "epoch 5, step 250,             l2_loss=2735.174, total_loss=2.389,             accuracy=0.312\n",
      "epoch 5, step 300,             l2_loss=2759.368, total_loss=2.114,             accuracy=0.406\n",
      "epoch 5, step 350,             l2_loss=2780.916, total_loss=1.447,             accuracy=0.625\n",
      "epoch 5, step 400,             l2_loss=2802.073, total_loss=1.799,             accuracy=0.344\n",
      "epoch 5, step 450,             l2_loss=2830.143, total_loss=2.054,             accuracy=0.406\n",
      "epoch 5, step 500,             l2_loss=2855.910, total_loss=2.053,             accuracy=0.469\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 5, val_loss=2.091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e540b79165d4274b9f1f1c5b258f9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, step 0,             l2_loss=2882.608, total_loss=2.183,             accuracy=0.406\n",
      "epoch 6, step 50,             l2_loss=2908.012, total_loss=1.517,             accuracy=0.594\n",
      "epoch 6, step 100,             l2_loss=2927.029, total_loss=2.028,             accuracy=0.438\n",
      "epoch 6, step 150,             l2_loss=2949.371, total_loss=1.462,             accuracy=0.594\n",
      "epoch 6, step 200,             l2_loss=2967.582, total_loss=1.505,             accuracy=0.531\n",
      "epoch 6, step 250,             l2_loss=2988.384, total_loss=1.960,             accuracy=0.438\n",
      "epoch 6, step 300,             l2_loss=3016.040, total_loss=1.641,             accuracy=0.594\n",
      "epoch 6, step 350,             l2_loss=3037.905, total_loss=1.215,             accuracy=0.594\n",
      "epoch 6, step 400,             l2_loss=3058.981, total_loss=1.643,             accuracy=0.500\n",
      "epoch 6, step 450,             l2_loss=3080.887, total_loss=1.478,             accuracy=0.438\n",
      "epoch 6, step 500,             l2_loss=3104.628, total_loss=1.373,             accuracy=0.594\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 6, val_loss=1.669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77807236221b4727a96ce240a501e48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, step 0,             l2_loss=3132.623, total_loss=1.731,             accuracy=0.469\n",
      "epoch 7, step 50,             l2_loss=3155.863, total_loss=0.914,             accuracy=0.688\n",
      "epoch 7, step 100,             l2_loss=3173.207, total_loss=1.670,             accuracy=0.469\n",
      "epoch 7, step 150,             l2_loss=3195.291, total_loss=1.455,             accuracy=0.625\n",
      "epoch 7, step 200,             l2_loss=3216.220, total_loss=1.212,             accuracy=0.656\n",
      "epoch 7, step 250,             l2_loss=3236.025, total_loss=1.877,             accuracy=0.500\n",
      "epoch 7, step 300,             l2_loss=3262.333, total_loss=1.667,             accuracy=0.594\n",
      "epoch 7, step 350,             l2_loss=3281.756, total_loss=1.183,             accuracy=0.594\n",
      "epoch 7, step 400,             l2_loss=3304.215, total_loss=1.463,             accuracy=0.500\n",
      "epoch 7, step 450,             l2_loss=3326.282, total_loss=1.673,             accuracy=0.562\n",
      "epoch 7, step 500,             l2_loss=3352.019, total_loss=1.666,             accuracy=0.562\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 7, val_loss=1.615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4b8b087aef424898838d2e10fc6807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, step 0,             l2_loss=3380.879, total_loss=1.459,             accuracy=0.531\n",
      "epoch 8, step 50,             l2_loss=3404.839, total_loss=0.944,             accuracy=0.719\n",
      "epoch 8, step 100,             l2_loss=3421.717, total_loss=1.635,             accuracy=0.531\n",
      "epoch 8, step 150,             l2_loss=3441.150, total_loss=1.417,             accuracy=0.562\n",
      "epoch 8, step 200,             l2_loss=3463.680, total_loss=1.081,             accuracy=0.719\n",
      "epoch 8, step 250,             l2_loss=3483.100, total_loss=1.648,             accuracy=0.469\n",
      "epoch 8, step 300,             l2_loss=3503.276, total_loss=1.537,             accuracy=0.625\n",
      "epoch 8, step 350,             l2_loss=3520.439, total_loss=0.885,             accuracy=0.656\n",
      "epoch 8, step 400,             l2_loss=3536.891, total_loss=1.651,             accuracy=0.500\n",
      "epoch 8, step 450,             l2_loss=3561.492, total_loss=1.736,             accuracy=0.500\n",
      "epoch 8, step 500,             l2_loss=3578.805, total_loss=1.327,             accuracy=0.562\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 8, val_loss=1.306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe540c797dfc4b30b97583a874dafe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, step 0,             l2_loss=3605.349, total_loss=1.420,             accuracy=0.562\n",
      "epoch 9, step 50,             l2_loss=3624.835, total_loss=0.796,             accuracy=0.781\n",
      "epoch 9, step 100,             l2_loss=3641.301, total_loss=1.197,             accuracy=0.594\n",
      "epoch 9, step 150,             l2_loss=3656.838, total_loss=1.064,             accuracy=0.688\n",
      "epoch 9, step 200,             l2_loss=3675.490, total_loss=1.199,             accuracy=0.688\n",
      "epoch 9, step 250,             l2_loss=3691.177, total_loss=1.433,             accuracy=0.469\n",
      "epoch 9, step 300,             l2_loss=3711.055, total_loss=1.400,             accuracy=0.562\n",
      "epoch 9, step 350,             l2_loss=3725.696, total_loss=0.777,             accuracy=0.781\n",
      "epoch 9, step 400,             l2_loss=3739.050, total_loss=1.395,             accuracy=0.625\n",
      "epoch 9, step 450,             l2_loss=3758.344, total_loss=0.986,             accuracy=0.719\n",
      "epoch 9, step 500,             l2_loss=3774.152, total_loss=1.406,             accuracy=0.562\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 9, val_loss=1.182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584c800b40cc494491d7f24476e20aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, step 0,             l2_loss=3794.368, total_loss=1.120,             accuracy=0.688\n",
      "epoch 10, step 50,             l2_loss=3814.518, total_loss=0.669,             accuracy=0.719\n",
      "epoch 10, step 100,             l2_loss=3832.568, total_loss=1.404,             accuracy=0.562\n",
      "epoch 10, step 150,             l2_loss=3848.726, total_loss=0.864,             accuracy=0.750\n",
      "epoch 10, step 200,             l2_loss=3868.152, total_loss=1.058,             accuracy=0.625\n",
      "epoch 10, step 250,             l2_loss=3884.509, total_loss=0.966,             accuracy=0.719\n",
      "epoch 10, step 300,             l2_loss=3900.419, total_loss=1.065,             accuracy=0.688\n",
      "epoch 10, step 350,             l2_loss=3915.320, total_loss=0.669,             accuracy=0.812\n",
      "epoch 10, step 400,             l2_loss=3928.321, total_loss=1.196,             accuracy=0.812\n",
      "epoch 10, step 450,             l2_loss=3947.436, total_loss=0.956,             accuracy=0.656\n",
      "epoch 10, step 500,             l2_loss=3963.096, total_loss=1.198,             accuracy=0.688\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 10, val_loss=1.053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720f4423f3284232aec11c59f52de1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, step 0,             l2_loss=3983.469, total_loss=0.943,             accuracy=0.688\n",
      "epoch 11, step 50,             l2_loss=4000.359, total_loss=0.737,             accuracy=0.719\n",
      "epoch 11, step 100,             l2_loss=4015.674, total_loss=1.100,             accuracy=0.656\n",
      "epoch 11, step 150,             l2_loss=4029.589, total_loss=0.802,             accuracy=0.656\n",
      "epoch 11, step 200,             l2_loss=4046.073, total_loss=1.144,             accuracy=0.594\n",
      "epoch 11, step 250,             l2_loss=4059.652, total_loss=0.811,             accuracy=0.688\n",
      "epoch 11, step 300,             l2_loss=4079.741, total_loss=0.942,             accuracy=0.719\n",
      "epoch 11, step 350,             l2_loss=4092.622, total_loss=0.545,             accuracy=0.906\n",
      "epoch 11, step 400,             l2_loss=4107.122, total_loss=1.455,             accuracy=0.562\n",
      "epoch 11, step 450,             l2_loss=4123.946, total_loss=1.308,             accuracy=0.656\n",
      "epoch 11, step 500,             l2_loss=4141.405, total_loss=1.001,             accuracy=0.812\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 11, val_loss=1.003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0805264113b344eb90076a909d211d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, step 0,             l2_loss=4159.882, total_loss=1.077,             accuracy=0.688\n",
      "epoch 12, step 50,             l2_loss=4174.384, total_loss=0.560,             accuracy=0.844\n",
      "epoch 12, step 100,             l2_loss=4186.660, total_loss=1.098,             accuracy=0.781\n",
      "epoch 12, step 150,             l2_loss=4200.418, total_loss=0.916,             accuracy=0.688\n",
      "epoch 12, step 200,             l2_loss=4218.945, total_loss=0.878,             accuracy=0.719\n",
      "epoch 12, step 250,             l2_loss=4233.623, total_loss=0.964,             accuracy=0.719\n",
      "epoch 12, step 300,             l2_loss=4248.164, total_loss=0.848,             accuracy=0.719\n",
      "epoch 12, step 350,             l2_loss=4262.421, total_loss=0.540,             accuracy=0.781\n",
      "epoch 12, step 400,             l2_loss=4276.633, total_loss=1.133,             accuracy=0.656\n",
      "epoch 12, step 450,             l2_loss=4290.013, total_loss=0.659,             accuracy=0.812\n",
      "epoch 12, step 500,             l2_loss=4302.478, total_loss=0.718,             accuracy=0.781\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 12, val_loss=0.804\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2bb75568854c12bfc1de4d90fd586d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, step 0,             l2_loss=4317.831, total_loss=1.029,             accuracy=0.688\n",
      "epoch 13, step 50,             l2_loss=4334.532, total_loss=0.282,             accuracy=0.906\n",
      "epoch 13, step 100,             l2_loss=4345.997, total_loss=0.922,             accuracy=0.656\n",
      "epoch 13, step 150,             l2_loss=4356.713, total_loss=0.660,             accuracy=0.812\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e687e5bdd97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# clip the grandient value for avoiding explosion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=0, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "# early stopping\n",
    "min_val_loss = np.Inf\n",
    "patience = 3\n",
    "global_step = 1\n",
    "writer = SummaryWriter('runs/experiment_{}'.format(datetime.now().strftime('%f')))\n",
    "\n",
    "model_ft.train()\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    for i, (img_batch, label_batch) in tqdm(enumerate(train_loader)):    \n",
    "        optimizer.zero_grad()      \n",
    "        img_batch = img_batch.to(device=device)\n",
    "        label_batch = label_batch.to(device=device)  \n",
    "        output = model_ft(img_batch)\n",
    "        loss = criterion(output, label_batch.squeeze())\n",
    "        \n",
    "        # l2 Regularization loss\n",
    "        l2_regularization = 0\n",
    "        for p in model.parameters():\n",
    "            l2_regularization += torch.norm(p, 2)\n",
    "#         loss = loss + 0.1 * l2_regularization\n",
    "\n",
    "        loss.backward()\n",
    "        # clip the grandient value for avoiding explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.9) \n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(output.cpu().data, 1)\n",
    "        accuracy = torch.sum(predicted == label_batch.cpu().data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "        \n",
    "        # Write tensorboard\n",
    "        writer.add_scalar('train/Accuracy', accuracy.item(), global_step)\n",
    "        writer.add_scalar('train/Loss', loss.item(), global_step)\n",
    "        writer.add_scalar('train/L2RegLoss', l2_regularization.item(), global_step)\n",
    "        writer.add_scalar('train/LR', get_lr(optimizer), global_step)\n",
    "                \n",
    "        global_step += 1\n",
    "        \n",
    "        if i % 50== 0:\n",
    "            print('epoch {}, step {}, \\\n",
    "            l2_loss={:.3f}, total_loss={:.3f}, \\\n",
    "            accuracy={:.3f}'.format(epoch+1, i, l2_regularization.item() , loss.item(), accuracy.item()))\n",
    "    \n",
    "    \n",
    "    print('--- Validation phase ---')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "            output = model(img_batch.to(device))\n",
    "            _, predicted = torch.max(output.cpu().data, 1)\n",
    "            loss = criterion(output, label_batch.to(device).squeeze())\n",
    "            accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Write tensorboard\n",
    "            writer.add_pr_curve('valid/pr_curve', label_batch.squeeze(), predicted.squeeze(), epoch*len(valid_loader)+i)\n",
    "            writer.add_images('valid/image_batch', img_batch, epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Accuracy', accuracy.item(), epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Loss', loss.item(), epoch*len(valid_loader)+i)\n",
    "    \n",
    "    eval_loss = eval_loss / len(valid_loader)\n",
    "    \n",
    "    scheduler.step(eval_loss)\n",
    "    \n",
    "    print('epoch {}, val_loss={:.3f}'.format(epoch+1, eval_loss))\n",
    "\n",
    "    ## Early Stopping\n",
    "    if eval_loss < min_val_loss:\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch+1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': eval_loss,\n",
    "            'optimizer' :optimizer.state_dict(),\n",
    "            }, 'ckpt/Transform/resNet_{}.ckpt'.format(epoch+1))\n",
    "        min_val_loss = eval_loss\n",
    "    else:\n",
    "        patience-=1\n",
    "    if patience == 0:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "        \n",
    "writer.close()\n",
    "print('Finish all training !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=0.7737226486206055\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = 'model_best.ckpt'\n",
    "model.load_state_dict(torch.load(CKPT_PATH)['state_dict'])\n",
    "model.eval()\n",
    "acc = 0\n",
    "for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "    output = model(img_batch.to(device))\n",
    "    _, predicted = torch.max(output.cpu().data, 1)\n",
    "    accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "    acc += accuracy\n",
    "print('accuracy={}'.format(acc/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
