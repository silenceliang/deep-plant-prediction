{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/silence/.pyenv/versions/3.7.2/envs/python-3.7.2/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader,SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'\n",
    "LABEL_DIR = 'label2.csv'\n",
    "IMAGE_DIR = os.path.join(DATA_DIR, 'DaanForestPark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 20\n",
    "FILTER_NUMS = 8\n",
    "FILTER_NUMS2 = 16\n",
    "CHANNEL_NUMS = 3\n",
    "KERNEL_SIZE = 13\n",
    "STRIDE = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToPILImage() -> Resize() -> ToTensor()\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "#         transforms.ColorJitter(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])        \n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DataSet and the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        _images, _labels = [], []\n",
    "        # total amount of dataset \n",
    "        _number = 0\n",
    "        # Reading the categorical file\n",
    "        label_df = pd.read_csv(label_dir)\n",
    "        \n",
    "        # Iterate all files including .jpg inages  \n",
    "        for subdir, dirs, files in tqdm(os.walk(image_dir)):\n",
    "            for filename in files:\n",
    "                corr_label = label_df[label_df['dirpath']==subdir[len(DATA_DIR)+1:]]['label'].values\n",
    "                if corr_label.size!= 0 and filename.endswith(('jpg')):\n",
    "                    _images.append(subdir + os.sep + filename)\n",
    "                    _labels.append(corr_label)\n",
    "                    _number+=1\n",
    "        \n",
    "        # Randomly arrange data pairs\n",
    "        mapIndexPosition = list(zip(_images, _labels))\n",
    "        random.shuffle(mapIndexPosition)\n",
    "        _images, _labels = zip(*mapIndexPosition)\n",
    "\n",
    "        self._image = iter(_images)\n",
    "        self._labels = iter(_labels)\n",
    "        self._number = _number\n",
    "        self._category = label_df['label'].nunique()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._number\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        img = next(self._image)\n",
    "        lab = next(self._labels)\n",
    "        \n",
    "        img = self._loadimage(img)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)        \n",
    "        return img, lab\n",
    "     \n",
    "    def _categorical(self, label):\n",
    "        return np.arange(self._category) == label[:,None]\n",
    "    \n",
    "    def _loadimage(self, file):\n",
    "        return Image.open(file).convert('RGB')\n",
    "    \n",
    "    def get_categorical_nums(self):\n",
    "        return self._category\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e6f6f3c43845daa8e62824c32d29f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MyDataset(IMAGE_DIR, LABEL_DIR, transform=transform)\n",
    "\n",
    "valid_size = .2\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, sampler=train_sampler, drop_last=True)\n",
    "valid_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, sampler=valid_sampler, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Conv2d:\n",
    "#         nn.init.xavier_uniform(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "#     if type(m) == nn.Linear:\n",
    "#         nn.init.uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)   \n",
    "# class SimpleCNN(nn.Module):\n",
    "    \n",
    "#     def __init__(self, target):\n",
    "#         super(SimpleCNN, self).__init__()\n",
    "#         # Input size (3, 1136, 640)\n",
    "# #         self.imgzipper  = nn.AvgPool2d(kernel_size=ZIPSIZE)\n",
    "#         # Input size (3, 284, 160)\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=CHANNEL_NUMS,\n",
    "#                         out_channels=FILTER_NUMS,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (8, 1136, 640)\n",
    "#             nn.MaxPool2d(kernel_size=KERNEL_SIZE)\n",
    "#             # (8, 87, 49)\n",
    "#             # zipper (8, 21, 12)\n",
    "#         ).apply(init_weights)\n",
    "        \n",
    "#         # (8, 87, 49)\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=FILTER_NUMS,\n",
    "#                         out_channels=FILTER_NUMS2,\n",
    "#                         kernel_size=KERNEL_SIZE,\n",
    "#                         stride=STRIDE,\n",
    "#                         padding=(KERNEL_SIZE-STRIDE)//2 # padding=(kernel_size-stride)/2 -> original size\n",
    "#                     ),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.ReLU(),\n",
    "#             # (16, 87, 49)\n",
    "#             nn.MaxPool2d(kernel_size=5)\n",
    "#             # (16, 6, 3)\n",
    "#         ).apply(init_weights)\n",
    "#         self.MLP = nn.Sequential(\n",
    "#             nn.Linear(128, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, 81),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(81, target)\n",
    "#         ).apply(init_weights)\n",
    "#     def forward(self, x):\n",
    "#         x = self.imgzipper(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.MLP(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleCNN(train_dataset.get_categorical_nums()).to(device)\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs, train_dataset.get_categorical_nums())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 64, 170, 96]           9,408\n",
      "       BatchNorm2d-2          [-1, 64, 170, 96]             128\n",
      "              ReLU-3          [-1, 64, 170, 96]               0\n",
      "         MaxPool2d-4           [-1, 64, 85, 48]               0\n",
      "            Conv2d-5           [-1, 64, 85, 48]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 85, 48]             128\n",
      "              ReLU-7           [-1, 64, 85, 48]               0\n",
      "            Conv2d-8           [-1, 64, 85, 48]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 85, 48]             128\n",
      "             ReLU-10           [-1, 64, 85, 48]               0\n",
      "       BasicBlock-11           [-1, 64, 85, 48]               0\n",
      "           Conv2d-12           [-1, 64, 85, 48]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 85, 48]             128\n",
      "             ReLU-14           [-1, 64, 85, 48]               0\n",
      "           Conv2d-15           [-1, 64, 85, 48]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 85, 48]             128\n",
      "             ReLU-17           [-1, 64, 85, 48]               0\n",
      "       BasicBlock-18           [-1, 64, 85, 48]               0\n",
      "           Conv2d-19          [-1, 128, 43, 24]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 43, 24]             256\n",
      "             ReLU-21          [-1, 128, 43, 24]               0\n",
      "           Conv2d-22          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 43, 24]             256\n",
      "           Conv2d-24          [-1, 128, 43, 24]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 43, 24]             256\n",
      "             ReLU-26          [-1, 128, 43, 24]               0\n",
      "       BasicBlock-27          [-1, 128, 43, 24]               0\n",
      "           Conv2d-28          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 43, 24]             256\n",
      "             ReLU-30          [-1, 128, 43, 24]               0\n",
      "           Conv2d-31          [-1, 128, 43, 24]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 43, 24]             256\n",
      "             ReLU-33          [-1, 128, 43, 24]               0\n",
      "       BasicBlock-34          [-1, 128, 43, 24]               0\n",
      "           Conv2d-35          [-1, 256, 22, 12]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 22, 12]             512\n",
      "             ReLU-37          [-1, 256, 22, 12]               0\n",
      "           Conv2d-38          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 22, 12]             512\n",
      "           Conv2d-40          [-1, 256, 22, 12]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 22, 12]             512\n",
      "             ReLU-42          [-1, 256, 22, 12]               0\n",
      "       BasicBlock-43          [-1, 256, 22, 12]               0\n",
      "           Conv2d-44          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 22, 12]             512\n",
      "             ReLU-46          [-1, 256, 22, 12]               0\n",
      "           Conv2d-47          [-1, 256, 22, 12]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 22, 12]             512\n",
      "             ReLU-49          [-1, 256, 22, 12]               0\n",
      "       BasicBlock-50          [-1, 256, 22, 12]               0\n",
      "           Conv2d-51           [-1, 512, 11, 6]       1,179,648\n",
      "      BatchNorm2d-52           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-53           [-1, 512, 11, 6]               0\n",
      "           Conv2d-54           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-55           [-1, 512, 11, 6]           1,024\n",
      "           Conv2d-56           [-1, 512, 11, 6]         131,072\n",
      "      BatchNorm2d-57           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-58           [-1, 512, 11, 6]               0\n",
      "       BasicBlock-59           [-1, 512, 11, 6]               0\n",
      "           Conv2d-60           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-61           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-62           [-1, 512, 11, 6]               0\n",
      "           Conv2d-63           [-1, 512, 11, 6]       2,359,296\n",
      "      BatchNorm2d-64           [-1, 512, 11, 6]           1,024\n",
      "             ReLU-65           [-1, 512, 11, 6]               0\n",
      "       BasicBlock-66           [-1, 512, 11, 6]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 85]          43,605\n",
      "================================================================\n",
      "Total params: 11,220,117\n",
      "Trainable params: 11,220,117\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 82.29\n",
      "Params size (MB): 42.80\n",
      "Estimated Total Size (MB): 125.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# channels, H, W\n",
    "model = model_ft.to(device=device)\n",
    "summary(model, input_size=(CHANNEL_NUMS, 340, 192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "def save_checkpoint(model, state, filename='runs/ckpt/model.ckpt'):\n",
    "    torch.save(state, filename)\n",
    "    torch.save(model, 'model_best.ckpt')\n",
    "#     shutil.copyfile(filename, 'model_best.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f649d7125240ccadc49022e636eafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, step 0,             l2_loss=681.806, total_loss=3.729,             accuracy=0.219\n",
      "epoch 1, step 50,             l2_loss=736.128, total_loss=3.263,             accuracy=0.156\n",
      "epoch 1, step 100,             l2_loss=737.171, total_loss=3.283,             accuracy=0.281\n",
      "epoch 1, step 150,             l2_loss=687.206, total_loss=2.853,             accuracy=0.281\n",
      "epoch 1, step 200,             l2_loss=667.961, total_loss=3.597,             accuracy=0.250\n",
      "epoch 1, step 250,             l2_loss=634.610, total_loss=2.953,             accuracy=0.312\n",
      "epoch 1, step 300,             l2_loss=704.059, total_loss=2.535,             accuracy=0.500\n",
      "epoch 1, step 350,             l2_loss=676.319, total_loss=3.100,             accuracy=0.250\n",
      "epoch 1, step 400,             l2_loss=646.924, total_loss=3.372,             accuracy=0.312\n",
      "epoch 1, step 450,             l2_loss=669.462, total_loss=2.754,             accuracy=0.469\n",
      "epoch 1, step 500,             l2_loss=705.002, total_loss=3.374,             accuracy=0.250\n",
      "\n",
      "--- Validation phase ---\n",
      "epoch 1, val_loss=2.650\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf1e38457194c5f89261c055b74c77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, step 0,             l2_loss=730.911, total_loss=3.463,             accuracy=0.250\n",
      "epoch 2, step 50,             l2_loss=759.719, total_loss=2.578,             accuracy=0.375\n",
      "epoch 2, step 100,             l2_loss=716.740, total_loss=2.917,             accuracy=0.281\n",
      "epoch 2, step 150,             l2_loss=708.380, total_loss=2.532,             accuracy=0.375\n",
      "epoch 2, step 200,             l2_loss=684.703, total_loss=2.765,             accuracy=0.469\n",
      "epoch 2, step 250,             l2_loss=671.303, total_loss=2.565,             accuracy=0.438\n",
      "epoch 2, step 300,             l2_loss=685.772, total_loss=2.320,             accuracy=0.500\n",
      "epoch 2, step 350,             l2_loss=684.139, total_loss=2.939,             accuracy=0.406\n",
      "epoch 2, step 400,             l2_loss=674.600, total_loss=2.749,             accuracy=0.469\n",
      "epoch 2, step 450,             l2_loss=713.320, total_loss=2.396,             accuracy=0.500\n",
      "epoch 2, step 500,             l2_loss=722.777, total_loss=3.215,             accuracy=0.344\n",
      "\n",
      "--- Validation phase ---\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=0, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss().to(device=device)\n",
    "\n",
    "# early stopping\n",
    "min_val_loss = np.Inf\n",
    "patience = 3\n",
    "global_step = 1\n",
    "write_file = 'runs/experiment_{}'.format(datetime.now().strftime('%f'))\n",
    "writer = SummaryWriter(write_file)\n",
    "os.mkdir(write_file + '/ckpt')\n",
    "\n",
    "model_ft.train()\n",
    "\n",
    "for epoch in range(EPOCHES):\n",
    "    for i, (img_batch, label_batch) in tqdm(enumerate(train_loader)):    \n",
    "        optimizer.zero_grad()      \n",
    "        img_batch = img_batch.to(device=device)\n",
    "        label_batch = label_batch.to(device=device)  \n",
    "        output = model_ft(img_batch)\n",
    "        loss = criterion(output, label_batch.squeeze())\n",
    "        \n",
    "        # l2 Regularization loss\n",
    "        l2_regularization = 0\n",
    "        for p in model.parameters():\n",
    "            l2_regularization += torch.norm(p, 2)\n",
    "        loss = loss + 1e-3 * l2_regularization\n",
    "\n",
    "        loss.backward()\n",
    "        # clip the grandient value for avoiding explosion\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.9) \n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(output.cpu().data, 1)\n",
    "        accuracy = torch.sum(predicted == label_batch.cpu().data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "        \n",
    "        # Write tensorboard\n",
    "        writer.add_scalar('train/Accuracy', accuracy.item(), global_step)\n",
    "        writer.add_scalar('train/Loss', loss.item(), global_step)\n",
    "        writer.add_scalar('train/L2RegLoss', l2_regularization.item(), global_step)\n",
    "        writer.add_scalar('train/LR', get_lr(optimizer), global_step)\n",
    "                \n",
    "        global_step += 1\n",
    "        \n",
    "        if i % 50== 0:\n",
    "            print('epoch {}, step {}, \\\n",
    "            l2_loss={:.3f}, total_loss={:.3f}, \\\n",
    "            accuracy={:.3f}'.format(epoch+1, i, l2_regularization.item() , loss.item(), accuracy.item()))\n",
    "    \n",
    "    \n",
    "    print('--- Validation phase ---')\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "            output = model(img_batch.to(device))\n",
    "            _, predicted = torch.max(output.cpu().data, 1)\n",
    "            loss = criterion(output, label_batch.to(device).squeeze())\n",
    "            accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # Write tensorboard\n",
    "            writer.add_pr_curve('valid/pr_curve', label_batch.squeeze(), predicted.squeeze(), epoch*len(valid_loader)+i)\n",
    "            writer.add_images('valid/image_batch', img_batch, epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Accuracy', accuracy.item(), epoch*len(valid_loader)+i)\n",
    "            writer.add_scalar('valid/Loss', loss.item(), epoch*len(valid_loader)+i)\n",
    "    \n",
    "    eval_loss = eval_loss / len(valid_loader)\n",
    "    \n",
    "    scheduler.step(eval_loss)\n",
    "    \n",
    "    print('epoch {}, val_loss={:.3f}'.format(epoch+1, eval_loss))\n",
    "\n",
    "    ## Early Stopping\n",
    "    if eval_loss < min_val_loss:\n",
    "        save_checkpoint(model, \n",
    "            {\n",
    "            'epoch': epoch+1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': eval_loss,\n",
    "            'optimizer' :optimizer.state_dict(),\n",
    "            }, os.path.join(write_file, 'ckpt/resNet_{}.ckpt'.format(epoch+1)))\n",
    "        min_val_loss = eval_loss\n",
    "    else:\n",
    "        patience-=1\n",
    "    if patience == 0:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "print('Finish all training !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "accuracy=0.7739507555961609\n"
     ]
    }
   ],
   "source": [
    "CKPT_PATH = 'model_best.ckpt'\n",
    "model.load_state_dict(torch.load(CKPT_PATH)['state_dict'])\n",
    "model.eval()\n",
    "acc = 0\n",
    "for i, (img_batch, label_batch) in enumerate(valid_loader):\n",
    "    output = model(img_batch.to(device))\n",
    "    _, predicted = torch.max(output.cpu().data, 1)\n",
    "    accuracy = torch.sum(predicted == label_batch.data.view(-1), dtype=torch.float32) / BATCH_SIZE\n",
    "    acc += accuracy\n",
    "print('accuracy={}'.format(acc/len(valid_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'whole_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
